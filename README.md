# Bayesian_GMM

---

#### EM algorithm (optimizer for frequentist GMM)
Expectation Maximization (EM) is an iterative algorithm used for finding local  maximum likelihood
estimation (MLE) for a probabilistic model `ϴ`, that incorporates latent or hidden variables (denoted
here with `z`).

`log(ϴ) = log P(x | ϴ) = log Σ P(x, z | ϴ)`. The above expression cannot be easily optimized via differentiation,
that is why EM is used.

Introduce any distribution/prior over the hidden variables `q(z)`, and applying Jensen's inequality
for concave functions (log is concave)`f(E[X]) >= E[f(x)]`. The inequality becomes:

(!summation is always for `z`)

`log P(x | ϴ) = log Σ q(z) * P(x, z | ϴ) = log E [P(x, z | ϴ) / q(z)` 
`>=`  `E[log P(x, z | ϴ) / q(z)`. The right term is called ELBO (evidence lower bound), will be written
as `F(q, ϴ)` and it gives a worst-case for the log-likelihood of the `q(z)` distribution.

`P(x, z | ϴ) = P(z | x, ϴ) * P(x | z, ϴ)`. By using this formula in `F(q, ϴ)`, it yields that `F(q, ϴ) =
Σ q(z) * log P(x | ϴ) + Σ q(z) * log P(z | x, ϴ) / q(z)`. The first summation is exactly the log-likelihood
of the dataset, `log P(x | ϴ)`, while the second is the Kullback-Leibler divergence  `KL( q(z) || P(z | x, ϴ)`
that measures how different  the probability distribution `q(z)` is from the true `P(z | x,  ϴ)`.
So, `F(q, ϴ) = log P(x | ϴ) - KL( q(z) || P(z | x, ϴ)`

The algorithm iteratively optimizes the ELBO, by keeping constant one of the terms, `ϴ` (E-step), then 
`q(z)` (M-step).

E-step: `q(z)_t = argmax over q(z) (F(q(z), ϴ_t))`, and this expression is maximized when `KL( q(z) || P(z | x, ϴ_t) = 0`, 
which happens when the 2 distributions are exactly the same, `q(z) = P(z | x, ϴ_t)`, which means `F(q, ϴ) =
E~P(z | x, ϴ)[log P(x, z | ϴ_t)]`. We introduce another auxiliary function `Q(ϴ| ϴ_t)` (0) which is the right_hand_side. 

With `ϴ_t` is denoted
the model parameters `ϴ` from `t`-th iteration

M-step: `ϴ_t+1 = argmax over ϴ (F(q(z)_t, ϴ)`

---
### Gaussian Mixture Model (GMM)
Given the number of gaussian components `j` as a hyperparameters, and the assumption that each point was generated by a
normal distribution `N`, GMM fits probabilistically, each data point to the given clusters. The hidden variables `z_ij` are an
encoding (usually 'one-hot') representing which cluster generated each `x_i`.

The parameters that are optimized in each iteration are: the means (`μ_j`) covariance matrices (`Σ_j`), and the weights `π_j` (sum over weights = 1) of each gaussian. 

E-step: given the current parameters (initialized randomly), what's the probability that each point belongs to each cluster ?
for the `i`-th data point it is calculated as `γ_ij` = `P(z_i = j | x_i, ϴ) =  π_j * N(x_i | μ, Σ) / Σ(sum over l) π_l * N (x_i | μ_l, Σ_l (cov_matrix))`

M-step: given the soft assignments, what are the best parameters of each component ?

Parameters updates for a multivariate gaussian:
`μ_j_t+1 = Σ (sum over i) γ_ij * x_i / Σ (sum over i) γ_ij` (1)

`π_j_t+1 = Σ (sum over i) γ_ij / n` (2), where n -the number of data points

`Σ_j (cov matrix) = (Σ (sum over i) γ_ij * (x_i - μ_j_t+1)(x_i + μ_j_t+1).T) / Σ (sum over i) γ_ij` (3), where T - means transposed 

(1) is deduced by taking the log of the d_dimensional gaussian PDF, and treating all the variables that are not
the parameters of the gaussian as constants

(2) is deduced by using the computed responsibilities(`γ_ij`) in equation (0) and are maximized w.r.t. `γ_ij`
using Lagrange's method, with the constraint that the mixing weights sum up to 1 and are non-negative.

(3) is deduced similarly to (1), but optimizing w.r.t the `Σ_j`, the covariance matrix.

---
### Bayesian GMM

The difference between the frequentist GMM and the Bayesian GMM is how the parameters of the gaussians are treated.
In GMM, the means and the covariance matrices are considered fixed, meanwhile in the Bayesian GMM these are random variables.

The task is to compute the intractable posterior P(π, μ, Σ, Z | X ). There are 2 methods: i) sampling (Gibbs sampling, MCMC)
and ii) VI (variational inference).

My implementation uses VI, which approximates the posterior using a 'simpler' tractable distribution, obtained by using
mean field approximation (assume independence between groups of probability distributions). P(π, μ, Σ, Z | X ) ~ q(π, μ, Σ, Z | X)

By using the multiplication rule q(π, μ, Σ, Z | X) = P(π) * P(μ, Σ | π) * P(Z | μ, Σ, π) * P (X | Z, μ, Σ, π)

Conjugate priors are used, in order to have closed-form updates (no sampling needed).

P(π) ~ Dirichlet (α), multivariate generalization of the Beta distribution, prior to the Categorical distribution z_i ~ Categorical(π)

the choice for Σ influences μ, such that we consider a hierarchical prior Normal Inverse Wishart (μ_0, k, v, Ψ)

first sample a covariance Σ ~ Inverse Wishart (v, Ψ) (probability distribution over positive definite matrices), 
with Wishart distribution being the multivariate generalization of the Gamma distribution
,then given a covariance Σ, sample a mean μ | Σ ~ Normal (μ_0, 1/k, Σ ).

The iterative optimization process is similar to EM and alternates between a i)variational E-step and a ii)variational M-step.

in i) update responsibilities r_ik = exp (E_qπ [log_πk]) + Σ_q(μ_k, Σ_k) [log P(x_i | μ_k, Σ_k)]

in ii) update the parameters of the Dirichlet distribution, as well as NIW
N_k (soft count) = Σ(over i) r_ik, x_k_bar = Σ(over i) r_ik * x_i / N_k, S_k = (Σ(over i) r_ik *(x_i - x_k_bar)*(x_i - x_k_bar).T) / N_k

Dirichlet distribution: α_k = α_prior_k + N_k
NIW: β_k = β_prior_k + N_k, μ_k = (β_prior_k * μ_prior + N_k * x_k_bar) / β_k, v_k = v_prior_k + N_k,
Ψ = Ψ_prior + N_k * S_k + (β_0 * N_k / β_k) * ((x_k_bar - μ_prior)*(x_k_bar - μ_prior).T)

---

One considerable advantage of the Bayesian GMM consists of  its ability to model the uncertainty of the data (and visualization: see Arviz),
instead of relying on hard parameters, although this comes with an increased complexity in the mathematical apparatus required for such computations.


