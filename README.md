# Bayesian_GMM

---

#### EM algorithm (optimizer for frequentist GMM)
Expectation Maximization (EM) is an iterative algorithm used for finding local  maximum likelihood
estimation (MLE) for a probabilistic model `ϴ`, that incorporates latent or hidden variables (denoted
here with `z`).

`log(ϴ) = log P(x | ϴ) = log Σ P(x, z | ϴ)`. The above expression cannot be easily optimized via differentiation,
that is why EM is used.

Introduce any distribution/prior over the hidden variables `q(z)`, and applying Jensen's inequality
for concave functions (log is concave)`f(E[X]) >= E[f(x)]`. The inequality becomes:

(!summation is always for `z`)

`log P(x | ϴ) = log Σ q(z) * P(x, z | ϴ) = log E [P(x, z | ϴ) / q(z)` 
`>=`  `E[log P(x, z | ϴ) / q(z)`. The right term is called ELBO (evidence lower bound), will be written
as `F(q, ϴ)` and it gives a worst-case for the log-likelihood of the `q(z)` distribution.

`P(x, z | ϴ) = P(z | x, ϴ) * P(x | z, ϴ)`. By using this formula in `F(q, ϴ)`, it yields that `F(q, ϴ) =
Σ q(z) * log P(x | ϴ) + Σ q(z) * log P(z | x, ϴ) / q(z)`. The first summation is exactly the log-likelihood
of the dataset, `log P(x | ϴ)`, while the second is the Kullback-Leibler divergence  `KL( q(z) || P(z | x, ϴ)`
that measures how different  the probability distribution `q(z)` is from the true `P(z | x,  ϴ)`.
So, `F(q, ϴ) = log P(x | ϴ) - KL( q(z) || P(z | x, ϴ)`

The algorithm iteratively optimizes the ELBO, by keeping constant one of the terms, `ϴ` (E-step), then 
`q(z)` (M-step).

E-step: `q(z)_t = argmax over q(z) (F(q(z), ϴ_t))`, and this expression is maximized when `KL( q(z) || P(z | x, ϴ_t) = 0`, 
which happens when the 2 distributions are exactly the same, `q(z) = P(z | x, ϴ_t)`, which means `F(q, ϴ) =
E~P(z | x, ϴ)[log P(x, z | ϴ_t)]`. We introduce another auxiliary function `Q(ϴ| ϴ_t)` (0) which is the right_hand_side. 

With `ϴ_t` is denoted
the model parameters `ϴ` from `t`-th iteration

M-step: `ϴ_t+1 = argmax over ϴ (F(q(z)_t, ϴ)`

---
### Gaussian Mixture Model (GMM)
Given the number of gaussian components `j` as a hyperparameters, and the assumption that each point was generated by a
normal distribution `N`, GMM fits probabilistically, each data point to the given clusters. The hidden variables `z_ij` are an
encoding (usually 'one-hot') representing which cluster generated each `x_i`.

The parameters that are optimized in each iteration are: the means (`μ_j`) covariance matrices (`Σ_j`), and the weights `π_j` (sum over weights = 1) of each gaussian. 

E-step: given the current parameters (initialized randomly), what's the probability that each point belongs to each cluster ?
for the `i`-th data point it is calculated as `γ_ij` = `P(z_i = j | x_i, ϴ) =  π_j * N(x_i | μ, Σ) / Σ(sum over l) π_l * N (x_i | μ_l, Σ_l (cov_matrix))`

M-step: given the soft assignments, what are the best parameters of each component ?

Parameters updates for a multivariate gaussian:
`μ_j_t+1 = Σ (sum over i) γ_ij * x_i / Σ (sum over i) γ_ij` (1)

`π_j_t+1 = Σ (sum over i) γ_ij / n` (2), where n -the number of data points

`Σ_j (cov matrix) = (Σ (sum over i) γ_ij * (x_i - μ_j_t+1)(x_i + μ_j_t+1).T) / Σ (sum over i) γ_ij` (3), where T - means transposed 

(1) is deduced by taking the log of the d_dimensional gaussian PDF, and treating all the variables that are not
the parameters of the gaussian as constants

(2) is deduced by using the computed responsibilities(`γ_ij`) in equation (0) and are maximized w.r.t. `γ_ij`
using Lagrange's method, with the constraint that the mixing weights sum up to 1 and are non-negative.

(3) is deduced similarly to (1), but optimizing w.r.t the `Σ_j`, the covariance matrix.

---
### Bayesian GMM
